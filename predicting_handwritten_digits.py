# -*- coding: utf-8 -*-
"""Predicting handwritten digits.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YzIswLpIYQaJoA8vZJRvCdT1bDzdPF87
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

print(f"Training data shape: {x_train.shape}") # 60,000 images of 28x28 pixels
print(f"Testing data shape: {x_test.shape}")   # 10,000 images

# Show the first image in the training set
plt.imshow(x_train[0], cmap='gray')
plt.title(f"Label: {y_train[0]}")
plt.show()

# 1. Flatten the 28x28 images into a 1D vector of 784 pixels
x_train = x_train.reshape(-1, 28 * 28)
x_test = x_test.reshape(-1, 28 * 28)

# 2. Normalize pixel values from [0, 255] to [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

print(f"New flattened shape: {x_train.shape}")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
    # Hidden Layer 1: 512 neurons, learns basic shapes
    Dense(512, activation='relu', input_shape=(784,)),

    # Hidden Layer 2: 256 neurons, learns more complex patterns
    Dense(256, activation='relu'),

    # Output Layer: 10 neurons (one for each digit 0-9)
    # Softmax turns the output into probabilities
    Dense(10, activation='softmax')
])

model.summary()

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model for 5 epochs (passes through the data)
history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1)

test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Final Test Accuracy: {test_acc * 100:.2f}%")

import matplotlib.pyplot as plt

# Plot Training & Validation Accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.show()

# Plot Training & Validation Loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

# 1. Get the model's predictions for the test set
y_pred = model.predict(x_test)
y_pred_classes = np.argmax(y_pred, axis=1) # Convert probabilities to class numbers

# 2. Create the confusion matrix
cm = confusion_matrix(y_test, y_pred_classes)

# 3. Visualize it using a Heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Initialize and train a simple Logistic Regression model
# We use a smaller subset or 'saga' solver for speed on 784 features
baseline_model = LogisticRegression(max_iter=100, solver='saga', tol=0.1)
baseline_model.fit(x_train, y_train)

# Evaluate
baseline_preds = baseline_model.predict(x_test)
baseline_acc = accuracy_score(y_test, baseline_preds)

print(f"MLP (Neural Network) Accuracy: {test_acc * 100:.2f}%")
print(f"Logistic Regression Accuracy: {baseline_acc * 100:.2f}%")

# Pick a random image from the test set
idx = 5
test_image = x_test[idx].reshape(1, 784) # Reshape for the model
true_label = y_test[idx]

# Predict
prediction = model.predict(test_image)
predicted_label = np.argmax(prediction)

print(f"True Label: {true_label}")
print(f"Model Prediction: {predicted_label}")

# Display the image
plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')
plt.title(f"Predicted: {predicted_label}, Actual: {true_label}")
plt.show()

# Save the model to a file
model.save('mnist_model.h5')

# Download it to your computer
from google.colab import files
files.download('mnist_model.h5')

